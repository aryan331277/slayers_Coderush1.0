{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e20338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting astropy\n",
      "  Downloading astropy-7.1.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astropy) (2.1.3)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy)\n",
      "  Downloading pyerfa-2.0.1.5-cp39-abi3-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting astropy-iers-data>=0.2025.4.28.0.37.27 (from astropy)\n",
      "  Downloading astropy_iers_data-0.2025.8.18.0.40.14-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astropy) (6.0.2)\n",
      "Requirement already satisfied: packaging>=22.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from astropy) (24.2)\n",
      "Downloading astropy-7.1.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.3 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.5 MB/s  0:00:01\n",
      "Downloading astropy_iers_data-0.2025.8.18.0.40.14-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.0/2.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.4 MB/s  0:00:00\n",
      "Downloading pyerfa-2.0.1.5-cp39-abi3-win_amd64.whl (349 kB)\n",
      "Installing collected packages: pyerfa, astropy-iers-data, astropy\n",
      "\n",
      "   ------------- -------------------------- 1/3 [astropy-iers-data]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   -------------------------- ------------- 2/3 [astropy]\n",
      "   ---------------------------------------- 3/3 [astropy]\n",
      "\n",
      "Successfully installed astropy-7.1.0 astropy-iers-data-0.2025.8.18.0.40.14 pyerfa-2.0.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install astropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a55e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_output.csv\n",
      "Total rows: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for CDF and FITS file processing\"\"\"\n",
    "    packages = ['cdflib', 'astropy', 'pandas', 'numpy']\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"‚úì {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úì {package} installed successfully\")\n",
    "\n",
    "install_packages()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import cdflib\n",
    "    from astropy.io import fits\n",
    "    print(\"\\nüéâ All required libraries imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please restart your Jupyter kernel and try again.\")\n",
    "\n",
    "def merge_cdf_fits_data(cdf_directory, fits_directory, output_csv=\"merged_data.csv\", \n",
    "                       time_aggregation=\"mean\", verbose=True):\n",
    "    \"\"\"\n",
    "    Merge CDF and FITS files based on timestamps in filenames.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cdf_directory : str\n",
    "        Path to directory containing CDF files\n",
    "    fits_directory : str  \n",
    "        Path to directory containing FITS files\n",
    "    output_csv : str\n",
    "        Output CSV filename (default: \"merged_data.csv\")\n",
    "    time_aggregation : str\n",
    "        How to handle multiple time values ('mean', 'first', 'last')\n",
    "    verbose : bool\n",
    "        Print detailed processing information\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Merged dataset with standardized variable names\n",
    "    \"\"\"\n",
    "\n",
    "    variables = {\n",
    "        'timestamp': ['EPOCH', 'Epoch', 'Time', 'epoch_for_cdf_mod'],\n",
    "        'velocity_x': ['proton_xvelocity', 'V_X', 'VX', 'VELOCITY_X'],\n",
    "        'velocity_y': ['proton_yvelocity', 'V_Y', 'VY', 'VELOCITY_Y'],\n",
    "        'velocity_z': ['proton_zvelocity', 'V_Z', 'VZ', 'VELOCITY_Z'],\n",
    "        'velocity_mag': ['bulk_p', 'proton_bulk_speed', 'V_MAG', 'V_TOTAL', 'SW_SPEED', 'v_p'],\n",
    "        'proton_density': ['numden_p', 'proton_density', 'N_P', 'PROTON_DENSITY', 'n_p'],\n",
    "        'proton_temp': ['thermal_p', 'proton_thermal', 'T_P', 'PROTON_TEMP', 'v_t_p'],\n",
    "        'alpha_density': ['numden_a', 'alpha_density', 'ALPHA_DENSITY', 'n_he'],\n",
    "        'alpha_speed': ['bulk_a', 'alpha_bulk_speed', 'ALPHA_SPEED', 'v_a'],\n",
    "        'alpha_temp': ['thermal_a', 'alpha_thermal', 'ALPHA_TEMP', 'v_t_a'],\n",
    "        'proton_density_uncertainty': ['numden_p_uncer'],\n",
    "        'proton_speed_uncertainty': ['bulk_p_uncer'],\n",
    "        'proton_temp_uncertainty': ['thermal_p_uncer'],\n",
    "        'alpha_density_uncertainty': ['numden_a_uncer'],\n",
    "        'alpha_speed_uncertainty': ['bulk_a_uncer'],\n",
    "        'alpha_temp_uncertainty': ['thermal_a_uncer'],\n",
    "        'spacecraft_x': ['spacecraft_xpos', 'sc_pos_x'],\n",
    "        'spacecraft_y': ['spacecraft_ypos', 'sc_pos_y'],\n",
    "        'spacecraft_z': ['spacecraft_zpos', 'sc_pos_z']\n",
    "    }\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        \"\"\"Extract YYYY-MM-DD timestamp from filename\"\"\"\n",
    "        patterns = [\n",
    "            r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD\n",
    "            r'(\\d{4}_\\d{2}_\\d{2})',  # YYYY_MM_DD\n",
    "            r'(\\d{8})',              # YYYYMMDD\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, filename)\n",
    "            if match:\n",
    "                date_str = match.group(1)\n",
    "                if '_' in date_str:\n",
    "                    return date_str.replace('_', '-')\n",
    "                elif len(date_str) == 8:\n",
    "                    return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "                else:\n",
    "                    return date_str\n",
    "        return None\n",
    "\n",
    "    def aggregate_array_data(data, method=\"mean\"):\n",
    "        \"\"\"Aggregate array data using specified method\"\"\"\n",
    "        if data is None:\n",
    "            return np.nan\n",
    "            \n",
    "        if not isinstance(data, (list, np.ndarray)) or len(data) == 0:\n",
    "            return data\n",
    "\n",
    "        try:\n",
    "            # Convert to numpy array if it isn't already\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                data = np.array(data)\n",
    "            \n",
    "            # Handle multi-dimensional arrays\n",
    "            if data.ndim > 1:\n",
    "                data = data.flatten()\n",
    "            \n",
    "            # Handle numeric data\n",
    "            if np.issubdtype(data.dtype, np.number):\n",
    "                # Remove inf and nan values for aggregation\n",
    "                finite_data = data[np.isfinite(data)]\n",
    "                if len(finite_data) == 0:\n",
    "                    return np.nan\n",
    "                    \n",
    "                if method == \"mean\":\n",
    "                    return np.nanmean(finite_data)\n",
    "                elif method == \"first\":\n",
    "                    return finite_data[0] if len(finite_data) > 0 else np.nan\n",
    "                elif method == \"last\":\n",
    "                    return finite_data[-1] if len(finite_data) > 0 else np.nan\n",
    "                else:\n",
    "                    return np.nanmean(finite_data)\n",
    "            else:\n",
    "                # Handle non-numeric data\n",
    "                return str(data[0]) if len(data) > 0 else \"\"\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Could not aggregate data: {e}\")\n",
    "            # Return first element as fallback\n",
    "            try:\n",
    "                if isinstance(data, (list, np.ndarray)) and len(data) > 0:\n",
    "                    return str(data[0]) if not isinstance(data[0], (int, float, np.number)) else data[0]\n",
    "                else:\n",
    "                    return data\n",
    "            except:\n",
    "                return str(data) if data is not None else \"\"\n",
    "\n",
    "    def read_cdf_file(filepath):\n",
    "        \"\"\"Read CDF file and return data dictionary\"\"\"\n",
    "        try:\n",
    "            cdf = cdflib.CDF(filepath)\n",
    "            data = {}\n",
    "            info = cdf.cdf_info()\n",
    "            all_vars = info.zVariables + info.rVariables\n",
    "\n",
    "            for var in all_vars:\n",
    "                try:\n",
    "                    var_data = cdf.varget(var)\n",
    "                    # Handle the case where varget returns None\n",
    "                    if var_data is not None:\n",
    "                        data[var] = var_data\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Warning: Could not read variable {var}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            cdf.close()\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error reading CDF file {filepath}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def read_fits_file(filepath):\n",
    "        \"\"\"Read FITS file and return data dictionary\"\"\"\n",
    "        try:\n",
    "            data = {}\n",
    "            with fits.open(filepath, ignore_missing_end=True) as hdul:\n",
    "                for i, hdu in enumerate(hdul):\n",
    "                    try:\n",
    "                        if hasattr(hdu, 'data') and hdu.data is not None:\n",
    "                            if hasattr(hdu.data, 'names') and hdu.data.names:\n",
    "                                for name in hdu.data.names:\n",
    "                                    try:\n",
    "                                        column_data = hdu.data[name]\n",
    "                                        # Handle masked arrays\n",
    "                                        if hasattr(column_data, 'filled'):\n",
    "                                            column_data = column_data.filled(np.nan)\n",
    "                                        data[name] = column_data\n",
    "                                    except Exception as e:\n",
    "                                        if verbose:\n",
    "                                            print(f\"Warning: Could not read column {name}: {e}\")\n",
    "                                        continue\n",
    "                            elif hasattr(hdu.data, 'shape') and len(hdu.data.shape) > 0:\n",
    "                                data[f'HDU_{i}_data'] = hdu.data\n",
    "\n",
    "                        if hasattr(hdu, 'header'):\n",
    "                            for key in hdu.header.keys():\n",
    "                                if key not in ['SIMPLE', 'BITPIX', 'NAXIS', 'NAXIS1', 'NAXIS2', \n",
    "                                             'EXTEND', 'COMMENT', 'HISTORY', '']:\n",
    "                                    try:\n",
    "                                        data[f'header_{key}'] = hdu.header[key]\n",
    "                                    except:\n",
    "                                        continue\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"Warning: Could not process HDU {i}: {e}\")\n",
    "                        continue\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error reading FITS file {filepath}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def standardize_variable_names(data, variables_mapping):\n",
    "        \"\"\"Standardize variable names based on mapping\"\"\"\n",
    "        standardized_data = {}\n",
    "        # Create a mapping of lowercase keys to original keys\n",
    "        data_keys_lower = {str(key).lower(): key for key in data.keys() if key is not None}\n",
    "\n",
    "        for standard_name, possible_names in variables_mapping.items():\n",
    "            for possible_name in possible_names:\n",
    "                possible_name_lower = possible_name.lower()\n",
    "                if possible_name_lower in data_keys_lower:\n",
    "                    original_key = data_keys_lower[possible_name_lower]\n",
    "                    standardized_data[standard_name] = data[original_key]\n",
    "                    break\n",
    "\n",
    "        # Add unmapped variables with a prefix to avoid conflicts\n",
    "        mapped_vars_lower = set()\n",
    "        for possible_names in variables_mapping.values():\n",
    "            mapped_vars_lower.update([name.lower() for name in possible_names])\n",
    "\n",
    "        for var_name, var_data in data.items():\n",
    "            if var_name is not None:\n",
    "                var_name_lower = str(var_name).lower()\n",
    "                if var_name_lower not in mapped_vars_lower:\n",
    "                    standardized_data[var_name] = var_data\n",
    "\n",
    "        return standardized_data\n",
    "\n",
    "    def process_directory(directory, file_extensions, read_function):\n",
    "        \"\"\"Process all files in directory with given extensions\"\"\"\n",
    "        file_data = {}\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"‚ùå Warning: Directory {directory} does not exist\")\n",
    "            return file_data\n",
    "\n",
    "        processed_count = 0\n",
    "        for filename in os.listdir(directory):\n",
    "            if any(filename.lower().endswith(ext.lower()) for ext in file_extensions):\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                if timestamp:\n",
    "                    filepath = os.path.join(directory, filename)\n",
    "                    if verbose:\n",
    "                        print(f\"üìÅ Processing {filename} (timestamp: {timestamp})\")\n",
    "\n",
    "                    raw_data = read_function(filepath)\n",
    "                    if raw_data:\n",
    "                        standardized_data = standardize_variable_names(raw_data, variables)\n",
    "\n",
    "                        # Apply aggregation to all data values\n",
    "                        for key, value in standardized_data.items():\n",
    "                            standardized_data[key] = aggregate_array_data(value, time_aggregation)\n",
    "\n",
    "                        file_data[timestamp] = standardized_data\n",
    "                        processed_count += 1\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f\"‚ö†Ô∏è Warning: No data extracted from {filename}\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Could not extract timestamp from {filename}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Successfully processed {processed_count} files from {directory}\")\n",
    "        return file_data\n",
    "    if verbose:\n",
    "        print(\"üöÄ Starting CDF and FITS File Merger\")\n",
    "        print(\"=\" * 60)\n",
    "    if not cdf_directory or not isinstance(cdf_directory, str):\n",
    "        print(\"‚ùå Error: Invalid CDF directory path\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not fits_directory or not isinstance(fits_directory, str):\n",
    "        print(\"‚ùå Error: Invalid FITS directory path\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cdf_data = process_directory(cdf_directory, ['.cdf'], read_cdf_file)\n",
    "    fits_data = process_directory(fits_directory, ['.fits', '.fit'], read_fits_file)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nüìä Found {len(cdf_data)} CDF files with valid timestamps\")\n",
    "        print(f\"üìä Found {len(fits_data)} FITS files with valid timestamps\")\n",
    "\n",
    "    common_timestamps = set(cdf_data.keys()) & set(fits_data.keys())\n",
    "    if verbose:\n",
    "        print(f\"üîó Found {len(common_timestamps)} common timestamps\")\n",
    "\n",
    "    all_timestamps = sorted(set(cdf_data.keys()) | set(fits_data.keys()))\n",
    "    merged_data = []\n",
    "\n",
    "    for timestamp in all_timestamps:\n",
    "        merged_row = {'date': timestamp}\n",
    "\n",
    "        if timestamp in cdf_data:\n",
    "            cdf_row = cdf_data[timestamp]\n",
    "            for key, value in cdf_row.items():\n",
    "                merged_row[f'cdf_{key}'] = value\n",
    "\n",
    "        if timestamp in fits_data:\n",
    "            fits_row = fits_data[timestamp]\n",
    "            for key, value in fits_row.items():\n",
    "                merged_row[f'fits_{key}'] = value\n",
    "\n",
    "        merged_data.append(merged_row)\n",
    "\n",
    "    if merged_data:\n",
    "        df = pd.DataFrame(merged_data)\n",
    "        if len(df) > 0:\n",
    "            df = df.sort_values('date').reset_index(drop=True)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    if verbose and len(df) > 0:\n",
    "        print(f\"\\n‚úÖ Created merged dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "        print(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    elif verbose:\n",
    "        print(\"\\n‚ö†Ô∏è No data was merged\")\n",
    "\n",
    "    if len(df) > 0:\n",
    "        try:\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            if verbose:\n",
    "                print(f\"üíæ Data saved to {output_csv}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving CSV file: {e}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"‚ö†Ô∏è No data to save to CSV\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"üéâ Merger function defined successfully!\")\n",
    "\n",
    "\n",
    "def check_directories(cdf_dir, fits_dir):\n",
    "    \"\"\"Check what files are in your directories\"\"\"\n",
    "    print(\"üîç Checking directories...\")\n",
    "\n",
    "    if os.path.exists(cdf_dir):\n",
    "        cdf_files = [f for f in os.listdir(cdf_dir) if f.lower().endswith('.cdf')]\n",
    "        print(f\"\\nüìÅ CDF Directory: {cdf_dir}\")\n",
    "        print(f\"   Found {len(cdf_files)} CDF files:\")\n",
    "        for f in cdf_files[:5]:  # Show first 5 files\n",
    "            print(f\"   - {f}\")\n",
    "        if len(cdf_files) > 5:\n",
    "            print(f\"   ... and {len(cdf_files)-5} more files\")\n",
    "    else:\n",
    "        print(f\"‚ùå CDF directory not found: {cdf_dir}\")\n",
    "\n",
    "    if os.path.exists(fits_dir):\n",
    "        fits_files = [f for f in os.listdir(fits_dir) if f.lower().endswith(('.fits', '.fit'))]\n",
    "        print(f\"\\nüìÅ FITS Directory: {fits_dir}\")\n",
    "        print(f\"   Found {len(fits_files)} FITS files:\")\n",
    "        for f in fits_files[:5]:  # Show first 5 files\n",
    "            print(f\"   - {f}\")\n",
    "        if len(fits_files) > 5:\n",
    "            print(f\"   ... and {len(fits_files)-5} more files\")\n",
    "    else:\n",
    "        print(f\"‚ùå FITS directory not found: {fits_dir}\")\n",
    "\n",
    "def analyze_merged_data(df):\n",
    "    \"\"\"Analyze the merged dataset\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"‚ùå No data to analyze\")\n",
    "        return\n",
    "\n",
    "    print(\"üìà Dataset Analysis:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    if 'date' in df.columns:\n",
    "        print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "    cdf_cols = [col for col in df.columns if col.startswith('cdf_')]\n",
    "    fits_cols = [col for col in df.columns if col.startswith('fits_')]\n",
    "\n",
    "    print(f\"   CDF variables: {len(cdf_cols)}\")\n",
    "    print(f\"   FITS variables: {len(fits_cols)}\")\n",
    "\n",
    "    standard_vars = ['timestamp', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                    'proton_density', 'proton_temp', 'velocity_mag']\n",
    "\n",
    "    print(\"\\nüéØ Standardized variables found:\")\n",
    "    for var in standard_vars:\n",
    "        cdf_found = f'cdf_{var}' in df.columns\n",
    "        fits_found = f'fits_{var}' in df.columns\n",
    "        status = \"\"\n",
    "        if cdf_found and fits_found:\n",
    "            status = \"‚úÖ Both CDF & FITS\"\n",
    "        elif cdf_found:\n",
    "            status = \"üîµ CDF only\"\n",
    "        elif fits_found:\n",
    "            status = \"üü° FITS only\"\n",
    "        else:\n",
    "            status = \"‚ùå Not found\"\n",
    "        print(f\"   {var}: {status}\")\n",
    "\n",
    "print(\"üìù Helper functions ready!\")\n",
    "\n",
    "print(\"üöÄ READY TO USE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. UPDATE YOUR PATHS:\")\n",
    "print(\"   Replace the example paths below with your actual directories\")\n",
    "print(\"\\n2. BASIC USAGE:\")\n",
    "print(\"   merged_df = merge_cdf_fits_data(cdf_path, fits_path)\")\n",
    "print(\"\\n3. ADVANCED USAGE:\")\n",
    "print(\"   merged_df = merge_cdf_fits_data(\")\n",
    "print(\"       cdf_directory=cdf_path,\")\n",
    "print(\"       fits_directory=fits_path,\")\n",
    "print(\"       output_csv='output.csv',\")\n",
    "print(\"       time_aggregation='mean',\")\n",
    "print(\"       verbose=True\")\n",
    "print(\"   )\")\n",
    "\n",
    "CDF_DIRECTORY = r\"D:\\Code Rush\\swis_2025Aug22T041821297\"\n",
    "FITS_DIRECTORY = r\"D:\\Code Rush\\suit_2025Aug22T054108230\"\n",
    "\n",
    "\n",
    "print(\"\\nüìã TEMPLATE CODE TO RUN:\")\n",
    "print(f\"# Check directories first:\")\n",
    "print(f\"check_directories('{CDF_DIRECTORY}', '{FITS_DIRECTORY}')\")\n",
    "print(f\"\\n# Run the merger:\")\n",
    "print(f\"merged_df = merge_cdf_fits_data('{CDF_DIRECTORY}', '{FITS_DIRECTORY}')\")\n",
    "print(f\"\\n# Analyze results:\")\n",
    "print(f\"analyze_merged_data(merged_df)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  REMEMBER TO:\")\n",
    "print(\"   - Update the directory paths above\")\n",
    "print(\"   - Make sure your files have timestamps in filenames (YYYY-MM-DD format)\")\n",
    "print(\"   - Check that both directories exist and contain the right file types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b28d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
